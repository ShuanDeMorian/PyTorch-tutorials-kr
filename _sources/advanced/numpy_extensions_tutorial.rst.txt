

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating extensions using numpy and scipy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of it’s implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of it’s implementation



.. code-block:: python


    import torch
    from torch.autograd import Function







Parameter-less example
----------------------

This layer doesn’t particularly do anything useful or mathematically
correct.

It is aptly named BadFFTFunction

**Layer Implementation**



.. code-block:: python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):

        def forward(self, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        def backward(self, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an nn.Module class


    def incorrect_fft(input):
        return BadFFTFunction()(input)







**Example usage of the created layer:**



.. code-block:: python


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 Out::

    tensor([[  3.6485,   5.2831,   4.9840,   8.7955,   4.8248],
            [  9.6334,   3.2467,   5.6585,  12.4690,   4.9455],
            [ 11.3583,   5.8000,  11.6500,   6.8201,   7.1760],
            [  4.8139,   5.8915,   4.3615,   9.6824,   7.9953],
            [ 13.4076,   9.0003,   3.5316,   8.6288,  10.6172],
            [  4.8139,   8.3110,   8.9100,   1.6618,   7.9953],
            [ 11.3583,   4.5544,   9.5831,   4.0977,   7.1760],
            [  9.6334,   3.0890,   2.7572,   6.8940,   4.9455]])
    tensor([[ 0.1677, -0.6591, -0.4118,  0.5657, -0.5300, -0.1199, -0.2343,
             -0.3398],
            [-1.0661, -0.2485,  0.5752,  0.2076, -1.3546,  0.9792,  0.8995,
             -0.6165],
            [ 0.9163, -0.8549, -0.3491, -1.0605,  0.4774,  0.0430,  0.0154,
              0.8942],
            [-1.8951, -0.4501,  0.3522,  0.2744,  0.5275,  2.0881, -1.7895,
              1.9408],
            [-1.0691,  0.0554, -0.6472, -1.7148, -0.3727, -1.9713, -0.7325,
              0.3086],
            [ 0.2190, -0.1154,  1.3101,  0.2479,  0.2041,  1.3190,  0.0019,
             -0.4261],
            [-0.5986, -0.2226,  0.1445, -0.2699,  1.3182,  0.4304,  0.9144,
              1.0276],
            [ 1.6219, -0.1950,  0.2612,  2.3336,  0.9982, -0.4480, -0.4620,
              1.2333]])


Parametrized example
--------------------

This implements a layer with learnable weights.

It implements the Cross-correlation with a learnable kernel.

In deep learning literature, it’s confusingly referred to as
Convolution.

The backward computes the gradients wrt the input and gradients wrt the
filter.

**Implementation:**

*Please Note that the implementation serves as an illustration, and we
did not verify it’s correctness*



.. code-block:: python


    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter):
            input, filter = input.detach(), filter.detach()  # detach so we can cast to NumPy
            result = correlate2d(input.numpy(), filter.detach().numpy(), mode='valid')
            ctx.save_for_backward(input, filter)
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter = ctx.saved_tensors
            grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full')
            grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid')

            return grad_output.new_tensor(grad_input), grad_output.new_tensor(grad_filter)


    class ScipyConv2d(Module):

        def __init__(self, kh, kw):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(kh, kw))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter)







**Example usage:**



.. code-block:: python


    module = ScipyConv2d(3, 3)
    print(list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print(output)
    output.backward(torch.randn(8, 8))
    print(input.grad)




.. rst-class:: sphx-glr-script-out

 Out::

    [Parameter containing:
    tensor([[-1.0582,  2.3049, -0.4000],
            [-0.0730,  0.2508,  0.3027],
            [ 0.5826,  0.9898,  1.1798]])]
    tensor([[-0.2151,  1.0500,  1.9991,  2.1783,  1.4561, -4.8820,  2.6387,
              1.2955],
            [ 3.9211,  1.3395, -2.4752, -0.0519,  0.3770,  0.4770,  2.6530,
              4.2775],
            [-0.5747, -2.5973, -0.3789,  1.7891,  0.0145,  0.9244,  5.6989,
             -3.4455],
            [-0.7912,  2.5237, -1.7735, -0.2666, -2.0779,  0.8552,  2.5705,
             -0.4175],
            [ 1.8567, -2.6877,  0.6385,  0.2817, -1.6383,  0.8097, -0.6141,
              2.7779],
            [-5.0677,  3.9776, -4.4833,  2.4090, -3.5810, -2.2156,  0.1891,
              0.6768],
            [-0.7446, -1.1529,  5.4451,  1.0311, -0.6268, -0.7796,  0.2365,
             -1.3021],
            [-3.8546, -1.4447, -0.8116,  0.7571,  1.5806, -1.4749, -2.9524,
             -0.7854]])
    tensor([[ 0.5385, -0.0055, -1.2288,  0.8731,  0.4757, -0.7207,  0.2046,
              1.2384, -0.0140, -0.6116],
            [ 0.6873,  0.7489,  0.7735, -2.3805,  0.4461, -0.3224,  0.2932,
             -2.7133, -0.6533, -0.7468],
            [-4.0049, -3.5399, -2.1427,  2.5904,  1.0714,  0.3052, -0.3047,
              0.9693, -0.7850, -0.7036],
            [ 0.2671,  1.7613, -3.5865,  0.0461, -1.9232, -3.7802, -0.5929,
             -2.5750, -0.5838,  1.1843],
            [ 0.7780,  1.1994,  1.5304, -0.4616, -5.0545, -2.8893, -2.4763,
              0.5672,  0.3687,  0.4832],
            [ 2.8392,  1.5644,  3.9382,  6.0490,  1.6432, -1.0574,  0.5106,
             -3.8807,  1.5488,  0.2376],
            [-3.2049,  2.1136, -5.4189, -0.8703, -1.6033, -5.1916,  3.4745,
              1.9739,  3.4909, -1.8777],
            [-1.5633, -1.6762,  3.2512, -4.7388,  0.1747, -0.2760,  4.8383,
             -2.5967,  2.6519, -1.0647],
            [ 4.9466, -2.3429,  2.6783, -1.9090,  1.7651, -0.7640, -1.3394,
              5.2317,  0.6924, -1.0516],
            [-0.8659,  1.2361,  1.8237, -1.4003,  0.7543, -0.5899,  0.7321,
              0.4139, -0.8858,  0.8945]])


**Total running time of the script:** ( 0 minutes  0.004 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
